; NOTE: Assertions have been autogenerated by test/update_tpde_llc_test_checks.py UTC_ARGS: --version 5
; SPDX-FileCopyrightText: 2025 Contributors to TPDE <https://tpde.org>
;
; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

; RUN: tpde-llc --target=x86_64 %s | %objdump | FileCheck %s -check-prefixes=X64
; RUN: tpde-llc --target=aarch64 %s | %objdump | FileCheck %s -check-prefixes=ARM64

define i8 @fshr_i8_3(i8 %a, i8 %b) {
; X64-LABEL: <fshr_i8_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x8
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov eax, 0x3
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and cl, 0x7
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i8_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w2, #0x18 // =24
; ARM64-NEXT:    lsl w1, w1, #24
; ARM64-NEXT:    orr w3, wzr, #0xfffffffc
; ARM64-NEXT:    mov x4, #0x3 // =3
; ARM64-NEXT:    bfxil w2, w4, #0, #3
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x3, x3, #0x7
; ARM64-NEXT:    lsr w2, w1, w2
; ARM64-NEXT:    lsl w1, w0, w3
; ARM64-NEXT:    orr w4, w1, w2
; ARM64-NEXT:    mov w0, w4
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %b, i8 3)
  ret i8 %res
}

define i8 @fshr_i8_221(i8 %a, i8 %b) {
; X64-LABEL: <fshr_i8_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x8
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov eax, 0xdd
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and cl, 0x7
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i8_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w2, #0x18 // =24
; ARM64-NEXT:    lsl w1, w1, #24
; ARM64-NEXT:    mov x3, #0xdd // =221
; ARM64-NEXT:    mvn w4, w3
; ARM64-NEXT:    bfxil w2, w3, #0, #3
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x4, x4, #0x7
; ARM64-NEXT:    lsr w2, w1, w2
; ARM64-NEXT:    lsl w1, w0, w4
; ARM64-NEXT:    orr w3, w1, w2
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %b, i8 221)
  ret i8 %res
}

define i8 @fshr_i8_dyn(i8 %a, i8 %b, i8 %c) {
; X64-LABEL: <fshr_i8_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x8
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    and cl, 0x7
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i8_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w3, #0x18 // =24
; ARM64-NEXT:    lsl w1, w1, #24
; ARM64-NEXT:    mvn w4, w2
; ARM64-NEXT:    bfxil w3, w2, #0, #3
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x4, x4, #0x7
; ARM64-NEXT:    lsr w3, w1, w3
; ARM64-NEXT:    lsl w1, w0, w4
; ARM64-NEXT:    orr w2, w1, w3
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %b, i8 %c)
  ret i8 %res
}

define i8 @fshr_rotate_i8_3(i8 %a) {
; X64-LABEL: <fshr_rotate_i8_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror dil, 0x3
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i8_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov x1, #0x3 // =3
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xff
; ARM64-NEXT:    and w1, w1, #0x7
; ARM64-NEXT:    and w2, w2, #0x7
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %a, i8 3)
  ret i8 %res
}

define i8 @fshr_rotate_i8_221(i8 %a) {
; X64-LABEL: <fshr_rotate_i8_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror dil, 0xdd
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i8_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov x1, #0xdd // =221
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xff
; ARM64-NEXT:    and w1, w1, #0x7
; ARM64-NEXT:    and w2, w2, #0x7
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %a, i8 221)
  ret i8 %res
}

define i8 @fshr_rotate_i8_dyn(i8 %a, i8 %c) {
; X64-LABEL: <fshr_rotate_i8_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, esi
; X64-NEXT:    ror dil, cl
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i8_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xff
; ARM64-NEXT:    and w1, w1, #0x7
; ARM64-NEXT:    and w2, w2, #0x7
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.fshr(i8 %a, i8 %a, i8 %c)
  ret i8 %res
}


define i16 @fshr_i16_3(i16 %a, i16 %b) {
; X64-LABEL: <fshr_i16_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x10
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov eax, 0x3
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and cl, 0xf
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i16_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w2, #0x10 // =16
; ARM64-NEXT:    lsl w1, w1, #16
; ARM64-NEXT:    orr w3, wzr, #0xfffffffc
; ARM64-NEXT:    mov x4, #0x3 // =3
; ARM64-NEXT:    bfxil w2, w4, #0, #4
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x3, x3, #0xf
; ARM64-NEXT:    lsr w2, w1, w2
; ARM64-NEXT:    lsl w1, w0, w3
; ARM64-NEXT:    orr w4, w1, w2
; ARM64-NEXT:    mov w0, w4
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %b, i16 3)
  ret i16 %res
}

define i16 @fshr_i16_221(i16 %a, i16 %b) {
; X64-LABEL: <fshr_i16_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x10
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov eax, 0xdd
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and cl, 0xf
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i16_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w2, #0x10 // =16
; ARM64-NEXT:    lsl w1, w1, #16
; ARM64-NEXT:    mov x3, #0xdd // =221
; ARM64-NEXT:    mvn w4, w3
; ARM64-NEXT:    bfxil w2, w3, #0, #4
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x4, x4, #0xf
; ARM64-NEXT:    lsr w2, w1, w2
; ARM64-NEXT:    lsl w1, w0, w4
; ARM64-NEXT:    orr w3, w1, w2
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %b, i16 221)
  ret i16 %res
}

define i16 @fshr_i16_dyn(i16 %a, i16 %b, i16 %c) {
; X64-LABEL: <fshr_i16_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x10
; X64-NEXT:    lea esi, [rdi + rsi]
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    and cl, 0xf
; X64-NEXT:    shr esi, cl
; X64-NEXT:    mov eax, esi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i16_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov w3, #0x10 // =16
; ARM64-NEXT:    lsl w1, w1, #16
; ARM64-NEXT:    mvn w4, w2
; ARM64-NEXT:    bfxil w3, w2, #0, #4
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    and x4, x4, #0xf
; ARM64-NEXT:    lsr w3, w1, w3
; ARM64-NEXT:    lsl w1, w0, w4
; ARM64-NEXT:    orr w2, w1, w3
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %b, i16 %c)
  ret i16 %res
}

define i16 @fshr_rotate_i16_3(i16 %a) {
; X64-LABEL: <fshr_rotate_i16_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror di, 0x3
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i16_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov x1, #0x3 // =3
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xffff
; ARM64-NEXT:    and w1, w1, #0xf
; ARM64-NEXT:    and w2, w2, #0xf
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %a, i16 3)
  ret i16 %res
}

define i16 @fshr_rotate_i16_221(i16 %a) {
; X64-LABEL: <fshr_rotate_i16_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror di, 0xdd
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i16_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    mov x1, #0xdd // =221
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xffff
; ARM64-NEXT:    and w1, w1, #0xf
; ARM64-NEXT:    and w2, w2, #0xf
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %a, i16 221)
  ret i16 %res
}

define i16 @fshr_rotate_i16_dyn(i16 %a, i16 %c) {
; X64-LABEL: <fshr_rotate_i16_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, esi
; X64-NEXT:    ror di, cl
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i16_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    neg w2, w1
; ARM64-NEXT:    and w3, w0, #0xffff
; ARM64-NEXT:    and w1, w1, #0xf
; ARM64-NEXT:    and w2, w2, #0xf
; ARM64-NEXT:    lsr w3, w3, w1
; ARM64-NEXT:    lsl w2, w0, w2
; ARM64-NEXT:    orr w0, w3, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.fshr(i16 %a, i16 %a, i16 %c)
  ret i16 %res
}


define i32 @fshr_i32_3(i32 %a, i32 %b) {
; X64-LABEL: <fshr_i32_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shr esi, 0x3
; X64-NEXT:    lea edi, [rdi + rdi]
; X64-NEXT:    mov eax, 0x3
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    not cl
; X64-NEXT:    shl edi, cl
; X64-NEXT:    or edi, esi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i32_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    orr w2, wzr, #0xfffffffc
; ARM64-NEXT:    lsr w1, w1, #3
; ARM64-NEXT:    lsl w0, w0, w2
; ARM64-NEXT:    orr w3, w0, w1
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %b, i32 3)
  ret i32 %res
}

define i32 @fshr_i32_221(i32 %a, i32 %b) {
; X64-LABEL: <fshr_i32_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shr esi, 0xdd
; X64-NEXT:    lea edi, [rdi + rdi]
; X64-NEXT:    mov eax, 0xdd
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    not cl
; X64-NEXT:    shl edi, cl
; X64-NEXT:    or edi, esi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i32_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    mov x2, #0xdd // =221
; ARM64-NEXT:    mvn w3, w2
; ARM64-NEXT:    lsr w1, w1, #29
; ARM64-NEXT:    lsl w0, w0, w3
; ARM64-NEXT:    orr w2, w0, w1
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %b, i32 221)
  ret i32 %res
}

define i32 @fshr_i32_dyn(i32 %a, i32 %b, i32 %c) {
; X64-LABEL: <fshr_i32_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    shr esi, cl
; X64-NEXT:    lea edi, [rdi + rdi]
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    not cl
; X64-NEXT:    shl edi, cl
; X64-NEXT:    or edi, esi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i32_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #1
; ARM64-NEXT:    mvn w3, w2
; ARM64-NEXT:    lsr w1, w1, w2
; ARM64-NEXT:    lsl w0, w0, w3
; ARM64-NEXT:    orr w2, w0, w1
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %b, i32 %c)
  ret i32 %res
}

define i32 @fshr_rotate_i32_3(i32 %a) {
; X64-LABEL: <fshr_rotate_i32_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror edi, 0x3
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i32_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror w0, w0, #0x3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %a, i32 3)
  ret i32 %res
}

define i32 @fshr_rotate_i32_221(i32 %a) {
; X64-LABEL: <fshr_rotate_i32_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror edi, 0xdd
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i32_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror w0, w0, #0x1d
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %a, i32 221)
  ret i32 %res
}

define i32 @fshr_rotate_i32_dyn(i32 %a, i32 %c) {
; X64-LABEL: <fshr_rotate_i32_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, esi
; X64-NEXT:    ror edi, cl
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i32_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror w0, w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.fshr(i32 %a, i32 %a, i32 %c)
  ret i32 %res
}


define i64 @fshr_i64_3(i64 %a, i64 %b) {
; X64-LABEL: <fshr_i64_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shr rsi, 0x3
; X64-NEXT:    lea rdi, [rdi + rdi]
; X64-NEXT:    mov eax, 0x3
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    not cl
; X64-NEXT:    shl rdi, cl
; X64-NEXT:    or rdi, rsi
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i64_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl x0, x0, #1
; ARM64-NEXT:    orr w2, wzr, #0xfffffffc
; ARM64-NEXT:    lsr x1, x1, #3
; ARM64-NEXT:    lsl x0, x0, x2
; ARM64-NEXT:    orr x3, x0, x1
; ARM64-NEXT:    mov x0, x3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %b, i64 3)
  ret i64 %res
}

define i64 @fshr_i64_221(i64 %a, i64 %b) {
; X64-LABEL: <fshr_i64_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shr rsi, 0xdd
; X64-NEXT:    lea rdi, [rdi + rdi]
; X64-NEXT:    mov eax, 0xdd
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    not cl
; X64-NEXT:    shl rdi, cl
; X64-NEXT:    or rdi, rsi
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i64_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl x0, x0, #1
; ARM64-NEXT:    mov x2, #0xdd // =221
; ARM64-NEXT:    mvn w3, w2
; ARM64-NEXT:    lsr x1, x1, #29
; ARM64-NEXT:    lsl x0, x0, x3
; ARM64-NEXT:    orr x2, x0, x1
; ARM64-NEXT:    mov x0, x2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %b, i64 221)
  ret i64 %res
}

define i64 @fshr_i64_dyn(i64 %a, i64 %b, i64 %c) {
; X64-LABEL: <fshr_i64_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    shr rsi, cl
; X64-NEXT:    lea rdi, [rdi + rdi]
; X64-NEXT:    mov ecx, edx
; X64-NEXT:    not cl
; X64-NEXT:    shl rdi, cl
; X64-NEXT:    or rdi, rsi
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_i64_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl x0, x0, #1
; ARM64-NEXT:    mvn w3, w2
; ARM64-NEXT:    lsr x1, x1, x2
; ARM64-NEXT:    lsl x0, x0, x3
; ARM64-NEXT:    orr x2, x0, x1
; ARM64-NEXT:    mov x0, x2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %b, i64 %c)
  ret i64 %res
}

define i64 @fshr_rotate_i64_3(i64 %a) {
; X64-LABEL: <fshr_rotate_i64_3>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror rdi, 0x3
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i64_3>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror x0, x0, #0x3
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %a, i64 3)
  ret i64 %res
}

define i64 @fshr_rotate_i64_221(i64 %a) {
; X64-LABEL: <fshr_rotate_i64_221>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    ror rdi, 0xdd
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i64_221>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror x0, x0, #0x1d
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %a, i64 221)
  ret i64 %res
}

define i64 @fshr_rotate_i64_dyn(i64 %a, i64 %c) {
; X64-LABEL: <fshr_rotate_i64_dyn>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    mov ecx, esi
; X64-NEXT:    ror rdi, cl
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <fshr_rotate_i64_dyn>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ror x0, x0, x1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.fshr(i64 %a, i64 %a, i64 %c)
  ret i64 %res
}

